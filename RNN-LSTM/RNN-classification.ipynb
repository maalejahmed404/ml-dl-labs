{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBFqSEkKqpCN"
      },
      "source": [
        "# Lab Deep Learning/ Recurrent Neural Networks/ in pytorch\n",
        "\n",
        "## Using Many-to-One for movie rating predicton\n",
        "\n",
        "**Author: created by geoffroy.peeters@telecom-paris.fr** with the help of Stéphane Lathuilière\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "## Objective:\n",
        "You will implement two different networks to perform the automatic rating (0 or 1) of movies given the text of their reviews.\n",
        "You will use the ```imdb``` (internet movie database) dataset.\n",
        "\n",
        "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmkCSNaXLqjh"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "cPEBJcoDzsR7",
        "outputId": "728a253e-6e16-4ecc-aad4-c07a9f96f4c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m518.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AOqjzDwioJj9"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from argparse import Namespace\n",
        "\n",
        "colab = True\n",
        "student = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5Yp4OQVvUtr"
      },
      "source": [
        "## Parameters of the model\n",
        "\n",
        "- We only consider the most used words in the word dictionary, we consider the top `param.n_word`\n",
        "- We truncate/zero-pad each review to a length `param.T_x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4C_Pv7rYvRkM"
      },
      "outputs": [],
      "source": [
        "param = Namespace()\n",
        "\n",
        "param.n_word = 5000 # --- input dimension\n",
        "param.T_x = 100 # --- review length\n",
        "param.index_word_from = 3 # --- indicate where the index start from (first index are used to indicate `PAD` `START` `UNK` tokens)\n",
        "\n",
        "param.n_embedding = 32 # --- dimension of the embedding\n",
        "param.n_lstm = 100 # --- dimension of the LSTM (for a<t> and c<t>)\n",
        "param.n_out = 1 # --- binary classification problem\n",
        "\n",
        "param.batch_size = 64\n",
        "param.lr = 0.001\n",
        "param.n_epoch = 8\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsNcRimyLzgP"
      },
      "source": [
        "## Import IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5Gfe1ex8oN8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72597ddd-23d9-418b-c392-6ec9ae1836d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# --- Import the IMDB data and only consider the ``param.n_word``` most used words\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=param.n_word, index_from=param.index_word_from )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dfumrf2C6cGU",
        "outputId": "c783e0b2-9f7d-4d2a-e11e-935423d110f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElvpPKT46tXa",
        "outputId": "865f76b2-2c0f-4374-a407-de5e32a43395"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 14,\n",
              " 22,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 973,\n",
              " 1622,\n",
              " 1385,\n",
              " 65,\n",
              " 458,\n",
              " 4468,\n",
              " 66,\n",
              " 3941,\n",
              " 4,\n",
              " 173,\n",
              " 36,\n",
              " 256,\n",
              " 5,\n",
              " 25,\n",
              " 100,\n",
              " 43,\n",
              " 838,\n",
              " 112,\n",
              " 50,\n",
              " 670,\n",
              " 2,\n",
              " 9,\n",
              " 35,\n",
              " 480,\n",
              " 284,\n",
              " 5,\n",
              " 150,\n",
              " 4,\n",
              " 172,\n",
              " 112,\n",
              " 167,\n",
              " 2,\n",
              " 336,\n",
              " 385,\n",
              " 39,\n",
              " 4,\n",
              " 172,\n",
              " 4536,\n",
              " 1111,\n",
              " 17,\n",
              " 546,\n",
              " 38,\n",
              " 13,\n",
              " 447,\n",
              " 4,\n",
              " 192,\n",
              " 50,\n",
              " 16,\n",
              " 6,\n",
              " 147,\n",
              " 2025,\n",
              " 19,\n",
              " 14,\n",
              " 22,\n",
              " 4,\n",
              " 1920,\n",
              " 4613,\n",
              " 469,\n",
              " 4,\n",
              " 22,\n",
              " 71,\n",
              " 87,\n",
              " 12,\n",
              " 16,\n",
              " 43,\n",
              " 530,\n",
              " 38,\n",
              " 76,\n",
              " 15,\n",
              " 13,\n",
              " 1247,\n",
              " 4,\n",
              " 22,\n",
              " 17,\n",
              " 515,\n",
              " 17,\n",
              " 12,\n",
              " 16,\n",
              " 626,\n",
              " 18,\n",
              " 2,\n",
              " 5,\n",
              " 62,\n",
              " 386,\n",
              " 12,\n",
              " 8,\n",
              " 316,\n",
              " 8,\n",
              " 106,\n",
              " 5,\n",
              " 4,\n",
              " 2223,\n",
              " 2,\n",
              " 16,\n",
              " 480,\n",
              " 66,\n",
              " 3785,\n",
              " 33,\n",
              " 4,\n",
              " 130,\n",
              " 12,\n",
              " 16,\n",
              " 38,\n",
              " 619,\n",
              " 5,\n",
              " 25,\n",
              " 124,\n",
              " 51,\n",
              " 36,\n",
              " 135,\n",
              " 48,\n",
              " 25,\n",
              " 1415,\n",
              " 33,\n",
              " 6,\n",
              " 22,\n",
              " 12,\n",
              " 215,\n",
              " 28,\n",
              " 77,\n",
              " 52,\n",
              " 5,\n",
              " 14,\n",
              " 407,\n",
              " 16,\n",
              " 82,\n",
              " 2,\n",
              " 8,\n",
              " 4,\n",
              " 107,\n",
              " 117,\n",
              " 2,\n",
              " 15,\n",
              " 256,\n",
              " 4,\n",
              " 2,\n",
              " 7,\n",
              " 3766,\n",
              " 5,\n",
              " 723,\n",
              " 36,\n",
              " 71,\n",
              " 43,\n",
              " 530,\n",
              " 476,\n",
              " 26,\n",
              " 400,\n",
              " 317,\n",
              " 46,\n",
              " 7,\n",
              " 4,\n",
              " 2,\n",
              " 1029,\n",
              " 13,\n",
              " 104,\n",
              " 88,\n",
              " 4,\n",
              " 381,\n",
              " 15,\n",
              " 297,\n",
              " 98,\n",
              " 32,\n",
              " 2071,\n",
              " 56,\n",
              " 26,\n",
              " 141,\n",
              " 6,\n",
              " 194,\n",
              " 2,\n",
              " 18,\n",
              " 4,\n",
              " 226,\n",
              " 22,\n",
              " 21,\n",
              " 134,\n",
              " 476,\n",
              " 26,\n",
              " 480,\n",
              " 5,\n",
              " 144,\n",
              " 30,\n",
              " 2,\n",
              " 18,\n",
              " 51,\n",
              " 36,\n",
              " 28,\n",
              " 224,\n",
              " 92,\n",
              " 25,\n",
              " 104,\n",
              " 4,\n",
              " 226,\n",
              " 65,\n",
              " 16,\n",
              " 38,\n",
              " 1334,\n",
              " 88,\n",
              " 12,\n",
              " 16,\n",
              " 283,\n",
              " 5,\n",
              " 16,\n",
              " 4472,\n",
              " 113,\n",
              " 103,\n",
              " 32,\n",
              " 15,\n",
              " 16,\n",
              " 2,\n",
              " 19,\n",
              " 178,\n",
              " 32]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MRPBUpg6hii",
        "outputId": "d2868b21-8dcf-46db-ce28-be61bf6889f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSc5LmksOLyr"
      },
      "source": [
        "## Data content\n",
        "\n",
        "- ```X_train``` and ```X_test``` are each a numpy array, shape=(25000,), of lists.\n",
        "  - Each list represent a review; it is a sequence (represented as a list) of indexes (position of each word in the dictionary)\n",
        "\n",
        "- ```y_train``` and ```y_test``` are each a numpy array, shape=(25000,) of intergers.\n",
        "  - Each integer represent the values 0 (bad movie) or 1 (good movie)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "WouODCPrtiuu",
        "outputId": "0b623108-4138-453b-ea2c-39c559dfda09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(X_train): <class 'numpy.ndarray'>\n",
            "number of training sequences: X_train.shape: (25000,)\n",
            "type(X_train[0]): <class 'list'>\n",
            "length of the first training sequence: len(X_train[0]): 218\n",
            "length of the second training sequence: len(X_train[1]): 189\n",
            "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "maximum length of a training sequence: 2494\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAALR5JREFUeJzt3XtwVGWe//FPCHRLhO4YIOlkCBhEgchNooZelcUhk4DR0RWrRBlgFKFgg7UQhZiVHyJuTVhYL4wX2ClX49aCiFuiIxnAEAyMGlBSRm6SEiZscKETBkwaEMIlz++P+eX8bAhCQm5PeL+qTlX6PN9z+jmPsfPhOZcOM8YYAQAAWKRDa3cAAACgoQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrdGztDjSX2tpaHTx4UF27dlVYWFhrdwcAAFwGY4yOHTumuLg4dehw8XmWdhtgDh48qPj4+NbuBgAAaIQDBw6oZ8+eF21vtwGma9eukv42AB6Pp5V7AwAALkcwGFR8fLzzd/xi2m2AqTtt5PF4CDAAAFjmUpd/cBEvAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHU6tnYHrmbXP5N3wbr9C9NboScAANiFGRgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgnQYFmKVLl2rw4MHyeDzyeDzy+/1au3at0z5y5EiFhYWFLNOmTQvZR3l5udLT0xUREaHo6GjNnj1bZ8+eDakpLCzUsGHD5Ha71bdvX+Xm5jb+CAEAQLvTsSHFPXv21MKFC3XjjTfKGKN33nlH999/v77++mvdfPPNkqQpU6ZowYIFzjYRERHOz+fOnVN6erp8Pp+++OILHTp0SBMnTlSnTp30u9/9TpJUVlam9PR0TZs2TcuXL1dBQYGeeOIJxcbGKi0trSmOGQAAWC7MGGOuZAdRUVFavHixJk+erJEjR2ro0KF65ZVX6q1du3at7r33Xh08eFAxMTGSpGXLlikrK0uHDx+Wy+VSVlaW8vLytHPnTme7cePGqaqqSuvWrbvsfgWDQXm9XlVXV8vj8VzJITab65/Ju2Dd/oXprdATAADahsv9+93oa2DOnTunlStX6sSJE/L7/c765cuXq3v37ho4cKCys7P1448/Om1FRUUaNGiQE14kKS0tTcFgULt27XJqUlJSQt4rLS1NRUVFP9ufmpoaBYPBkAUAALRPDTqFJEk7duyQ3+/XqVOn1KVLF61evVqJiYmSpEcffVS9e/dWXFyctm/frqysLJWWluqDDz6QJAUCgZDwIsl5HQgEfrYmGAzq5MmT6ty5c739ysnJ0fPPP9/QwwEAABZqcIDp16+fSkpKVF1drf/+7//WpEmTtGnTJiUmJmrq1KlO3aBBgxQbG6tRo0Zp3759uuGGG5q04+fLzs5WZmam8zoYDCo+Pr5Z3xMAALSOBp9Ccrlc6tu3r5KSkpSTk6MhQ4ZoyZIl9dYmJydLkvbu3StJ8vl8qqioCKmpe+3z+X62xuPxXHT2RZLcbrdzd1TdAgAA2qcrfg5MbW2tampq6m0rKSmRJMXGxkqS/H6/duzYocrKSqcmPz9fHo/HOQ3l9/tVUFAQsp/8/PyQ62wAAMDVrUGnkLKzszVmzBj16tVLx44d04oVK1RYWKj169dr3759WrFihe655x5169ZN27dv16xZszRixAgNHjxYkpSamqrExERNmDBBixYtUiAQ0Ny5c5WRkSG32y1JmjZtml577TXNmTNHjz/+uDZu3KhVq1YpL+/CO3YAAMDVqUEBprKyUhMnTtShQ4fk9Xo1ePBgrV+/Xr/61a904MABbdiwQa+88opOnDih+Ph4jR07VnPnznW2Dw8P15o1azR9+nT5/X5de+21mjRpUshzYxISEpSXl6dZs2ZpyZIl6tmzp958802eAQMAABxX/ByYtornwAAAYJ9mfw4MAABAayHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACs0+Bvo0bzOv/hdjzYDgCACzEDAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6zQowCxdulSDBw+Wx+ORx+OR3+/X2rVrnfZTp04pIyND3bp1U5cuXTR27FhVVFSE7KO8vFzp6emKiIhQdHS0Zs+erbNnz4bUFBYWatiwYXK73erbt69yc3Mbf4QAAKDdaVCA6dmzpxYuXKji4mJt27ZNv/zlL3X//fdr165dkqRZs2bp448/1vvvv69Nmzbp4MGDevDBB53tz507p/T0dJ0+fVpffPGF3nnnHeXm5mrevHlOTVlZmdLT03X33XerpKREM2fO1BNPPKH169c30SEDAADbhRljzJXsICoqSosXL9ZDDz2kHj16aMWKFXrooYckSXv27NGAAQNUVFSk4cOHa+3atbr33nt18OBBxcTESJKWLVumrKwsHT58WC6XS1lZWcrLy9POnTud9xg3bpyqqqq0bt26y+5XMBiU1+tVdXW1PB7PlRxis7n+mbxL1uxfmN4CPQEAoG243L/fjb4G5ty5c1q5cqVOnDghv9+v4uJinTlzRikpKU5N//791atXLxUVFUmSioqKNGjQICe8SFJaWpqCwaAzi1NUVBSyj7qaun1cTE1NjYLBYMgCAADapwYHmB07dqhLly5yu92aNm2aVq9ercTERAUCAblcLkVGRobUx8TEKBAISJICgUBIeKlrr2v7uZpgMKiTJ09etF85OTnyer3OEh8f39BDAwAAlmhwgOnXr59KSkq0detWTZ8+XZMmTdLu3bubo28Nkp2drerqamc5cOBAa3cJAAA0k44N3cDlcqlv376SpKSkJH311VdasmSJHn74YZ0+fVpVVVUhszAVFRXy+XySJJ/Ppy+//DJkf3V3Kf205vw7lyoqKuTxeNS5c+eL9svtdsvtdjf0cAAAgIWu+DkwtbW1qqmpUVJSkjp16qSCggKnrbS0VOXl5fL7/ZIkv9+vHTt2qLKy0qnJz8+Xx+NRYmKiU/PTfdTV1O0DAACgQTMw2dnZGjNmjHr16qVjx45pxYoVKiws1Pr16+X1ejV58mRlZmYqKipKHo9HTz75pPx+v4YPHy5JSk1NVWJioiZMmKBFixYpEAho7ty5ysjIcGZPpk2bptdee01z5szR448/ro0bN2rVqlXKy7v0HTsAAODq0KAAU1lZqYkTJ+rQoUPyer0aPHiw1q9fr1/96leSpJdfflkdOnTQ2LFjVVNTo7S0NL3xxhvO9uHh4VqzZo2mT58uv9+va6+9VpMmTdKCBQucmoSEBOXl5WnWrFlasmSJevbsqTfffFNpaWlNdMgAAMB2V/wcmLaK58AAAGCfZn8ODAAAQGshwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOg0KMDk5ObrtttvUtWtXRUdH64EHHlBpaWlIzciRIxUWFhayTJs2LaSmvLxc6enpioiIUHR0tGbPnq2zZ8+G1BQWFmrYsGFyu93q27evcnNzG3eEAACg3WlQgNm0aZMyMjK0ZcsW5efn68yZM0pNTdWJEydC6qZMmaJDhw45y6JFi5y2c+fOKT09XadPn9YXX3yhd955R7m5uZo3b55TU1ZWpvT0dN19990qKSnRzJkz9cQTT2j9+vVXeLgAAKA96NiQ4nXr1oW8zs3NVXR0tIqLizVixAhnfUREhHw+X737+OSTT7R7925t2LBBMTExGjp0qF544QVlZWVp/vz5crlcWrZsmRISEvTiiy9KkgYMGKDPPvtML7/8stLS0hp6jAAAoJ25omtgqqurJUlRUVEh65cvX67u3btr4MCBys7O1o8//ui0FRUVadCgQYqJiXHWpaWlKRgMateuXU5NSkpKyD7T0tJUVFR00b7U1NQoGAyGLO3B9c/kXbAAAHC1a9AMzE/V1tZq5syZuuOOOzRw4EBn/aOPPqrevXsrLi5O27dvV1ZWlkpLS/XBBx9IkgKBQEh4keS8DgQCP1sTDAZ18uRJde7c+YL+5OTk6Pnnn2/s4QAAAIs0OsBkZGRo586d+uyzz0LWT5061fl50KBBio2N1ahRo7Rv3z7dcMMNje/pJWRnZyszM9N5HQwGFR8f32zvBwAAWk+jTiHNmDFDa9as0aeffqqePXv+bG1ycrIkae/evZIkn8+nioqKkJq613XXzVysxuPx1Dv7Iklut1sejydkAQAA7VODAowxRjNmzNDq1au1ceNGJSQkXHKbkpISSVJsbKwkye/3a8eOHaqsrHRq8vPz5fF4lJiY6NQUFBSE7Cc/P19+v78h3QUAAO1UgwJMRkaG/uu//ksrVqxQ165dFQgEFAgEdPLkSUnSvn379MILL6i4uFj79+/XH//4R02cOFEjRozQ4MGDJUmpqalKTEzUhAkT9M0332j9+vWaO3euMjIy5Ha7JUnTpk3TX/7yF82ZM0d79uzRG2+8oVWrVmnWrFlNfPgAAMBGDQowS5cuVXV1tUaOHKnY2Fhnee+99yRJLpdLGzZsUGpqqvr376+nnnpKY8eO1ccff+zsIzw8XGvWrFF4eLj8fr9+85vfaOLEiVqwYIFTk5CQoLy8POXn52vIkCF68cUX9eabb3ILNQAAkCSFGWNMa3eiOQSDQXm9XlVXV7fZ62Eae0v0/oXpTdwTAADahsv9+813IQEAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgnQYFmJycHN12223q2rWroqOj9cADD6i0tDSk5tSpU8rIyFC3bt3UpUsXjR07VhUVFSE15eXlSk9PV0REhKKjozV79mydPXs2pKawsFDDhg2T2+1W3759lZub27gjBAAA7U6DAsymTZuUkZGhLVu2KD8/X2fOnFFqaqpOnDjh1MyaNUsff/yx3n//fW3atEkHDx7Ugw8+6LSfO3dO6enpOn36tL744gu98847ys3N1bx585yasrIypaen6+6771ZJSYlmzpypJ554QuvXr2+CQwYAALYLM8aYxm58+PBhRUdHa9OmTRoxYoSqq6vVo0cPrVixQg899JAkac+ePRowYICKioo0fPhwrV27Vvfee68OHjyomJgYSdKyZcuUlZWlw4cPy+VyKSsrS3l5edq5c6fzXuPGjVNVVZXWrVt3WX0LBoPyer2qrq6Wx+Np7CE2q+ufyWvUdvsXpjdxTwAAaBsu9+/3FV0DU11dLUmKioqSJBUXF+vMmTNKSUlxavr3769evXqpqKhIklRUVKRBgwY54UWS0tLSFAwGtWvXLqfmp/uoq6nbBwAAuLp1bOyGtbW1mjlzpu644w4NHDhQkhQIBORyuRQZGRlSGxMTo0Ag4NT8NLzUtde1/VxNMBjUyZMn1blz5wv6U1NTo5qaGud1MBhs7KEBAIA2rtEzMBkZGdq5c6dWrlzZlP1ptJycHHm9XmeJj49v7S4BAIBm0qgAM2PGDK1Zs0affvqpevbs6az3+Xw6ffq0qqqqQuorKirk8/mcmvPvSqp7fakaj8dT7+yLJGVnZ6u6utpZDhw40JhDAwAAFmhQgDHGaMaMGVq9erU2btyohISEkPakpCR16tRJBQUFzrrS0lKVl5fL7/dLkvx+v3bs2KHKykqnJj8/Xx6PR4mJiU7NT/dRV1O3j/q43W55PJ6QBQAAtE8NugYmIyNDK1as0EcffaSuXbs616x4vV517txZXq9XkydPVmZmpqKiouTxePTkk0/K7/dr+PDhkqTU1FQlJiZqwoQJWrRokQKBgObOnauMjAy53W5J0rRp0/Taa69pzpw5evzxx7Vx40atWrVKeXmNu2sHAAC0Lw2agVm6dKmqq6s1cuRIxcbGOst7773n1Lz88su69957NXbsWI0YMUI+n08ffPCB0x4eHq41a9YoPDxcfr9fv/nNbzRx4kQtWLDAqUlISFBeXp7y8/M1ZMgQvfjii3rzzTeVlpbWBIcMAABsd0XPgWnLeA4MAAD2aZHnwAAAALQGAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOs0+ssc0XrOv/2a26oBAFcbZmAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKzT4ACzefNm3XfffYqLi1NYWJg+/PDDkPbf/va3CgsLC1lGjx4dUnP06FGNHz9eHo9HkZGRmjx5so4fPx5Ss337dt1111265pprFB8fr0WLFjX86AAAQLvU4ABz4sQJDRkyRK+//vpFa0aPHq1Dhw45y7vvvhvSPn78eO3atUv5+flas2aNNm/erKlTpzrtwWBQqamp6t27t4qLi7V48WLNnz9ff/jDHxraXQAA0A51bOgGY8aM0ZgxY362xu12y+fz1dv27bffat26dfrqq6906623SpJeffVV3XPPPfq3f/s3xcXFafny5Tp9+rTeeustuVwu3XzzzSopKdFLL70UEnQAAMDVqVmugSksLFR0dLT69eun6dOn68iRI05bUVGRIiMjnfAiSSkpKerQoYO2bt3q1IwYMUIul8upSUtLU2lpqX744Yd637OmpkbBYDBkAQAA7VOTB5jRo0frP//zP1VQUKB//dd/1aZNmzRmzBidO3dOkhQIBBQdHR2yTceOHRUVFaVAIODUxMTEhNTUva6rOV9OTo68Xq+zxMfHN/WhAQCANqLBp5AuZdy4cc7PgwYN0uDBg3XDDTeosLBQo0aNauq3c2RnZyszM9N5HQwGCTEAALRTzX4bdZ8+fdS9e3ft3btXkuTz+VRZWRlSc/bsWR09etS5bsbn86mioiKkpu71xa6tcbvd8ng8IQsAAGifmj3AfP/99zpy5IhiY2MlSX6/X1VVVSouLnZqNm7cqNraWiUnJzs1mzdv1pkzZ5ya/Px89evXT9ddd11zdxkAALRxDQ4wx48fV0lJiUpKSiRJZWVlKikpUXl5uY4fP67Zs2dry5Yt2r9/vwoKCnT//ferb9++SktLkyQNGDBAo0eP1pQpU/Tll1/q888/14wZMzRu3DjFxcVJkh599FG5XC5NnjxZu3bt0nvvvaclS5aEnCICAABXrwYHmG3btumWW27RLbfcIknKzMzULbfconnz5ik8PFzbt2/Xr3/9a910002aPHmykpKS9Oc//1lut9vZx/Lly9W/f3+NGjVK99xzj+68886QZ7x4vV598sknKisrU1JSkp566inNmzePW6gBAIAkKcwYY1q7E80hGAzK6/Wqurq6zV4Pc/0zeU2yn/0L05tkPwAAtLbL/fvd5HchoeXVF4QINQCA9owvcwQAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA63UbegpnruCwAAVztmYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOnyVQDt1/tcW7F+Y3ko9AQCg6TEDAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDs+BaSbnP4cFAAA0HWZgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADWIcAAAADrEGAAAIB1GhxgNm/erPvuu09xcXEKCwvThx9+GNJujNG8efMUGxurzp07KyUlRd99911IzdGjRzV+/Hh5PB5FRkZq8uTJOn78eEjN9u3bddddd+maa65RfHy8Fi1a1PCjAwAA7VKDA8yJEyc0ZMgQvf766/W2L1q0SL///e+1bNkybd26Vddee63S0tJ06tQpp2b8+PHatWuX8vPztWbNGm3evFlTp0512oPBoFJTU9W7d28VFxdr8eLFmj9/vv7whz804hABAEB7E2aMMY3eOCxMq1ev1gMPPCDpb7MvcXFxeuqpp/T0009LkqqrqxUTE6Pc3FyNGzdO3377rRITE/XVV1/p1ltvlSStW7dO99xzj77//nvFxcVp6dKlevbZZxUIBORyuSRJzzzzjD788EPt2bPnsvoWDAbl9XpVXV0tj8fT2ENstLb2XUj7F6a3dhcAALiky/373aTXwJSVlSkQCCglJcVZ5/V6lZycrKKiIklSUVGRIiMjnfAiSSkpKerQoYO2bt3q1IwYMcIJL5KUlpam0tJS/fDDD/W+d01NjYLBYMgCAADapyYNMIFAQJIUExMTsj4mJsZpCwQCio6ODmnv2LGjoqKiQmrq28dP3+N8OTk58nq9zhIfH3/lBwQAANqkdnMXUnZ2tqqrq53lwIEDrd0lAADQTJo0wPh8PklSRUVFyPqKigqnzefzqbKyMqT97NmzOnr0aEhNffv46Xucz+12y+PxhCwAAKB96tiUO0tISJDP51NBQYGGDh0q6W8X42zdulXTp0+XJPn9flVVVam4uFhJSUmSpI0bN6q2tlbJyclOzbPPPqszZ86oU6dOkqT8/Hz169dP1113XVN2+apR30XFXNgLALBVg2dgjh8/rpKSEpWUlEj624W7JSUlKi8vV1hYmGbOnKl/+Zd/0R//+Eft2LFDEydOVFxcnHOn0oABAzR69GhNmTJFX375pT7//HPNmDFD48aNU1xcnCTp0Ucflcvl0uTJk7Vr1y699957WrJkiTIzM5vswAEAgL0aPAOzbds23X333c7rulAxadIk5ebmas6cOTpx4oSmTp2qqqoq3XnnnVq3bp2uueYaZ5vly5drxowZGjVqlDp06KCxY8fq97//vdPu9Xr1ySefKCMjQ0lJSerevbvmzZsX8qwYAABw9bqi58C0ZTwH5tI4hQQAaGta5TkwAAAALYEAAwAArNOkdyFdzWw4ZQQAQHvBDAwAALAOAQYAAFiHAAMAAKxDgAEAANYhwAAAAOsQYAAAgHUIMAAAwDoEGAAAYB0CDAAAsA4BBgAAWIcAAwAArEOAAQAA1uHLHK9i9X0B5f6F6a3QEwAAGoYZGAAAYB0CDAAAsA4BBgAAWIdrYBDi/OtiuCYGANAWMQMDAACsQ4ABAADWIcAAAADrEGAAAIB1CDAAAMA6BBgAAGAdAgwAALAOAQYAAFiHAAMAAKxDgAEAANbhqwTws87/agGJrxcAALQ+ZmAAAIB1CDAAAMA6BBgAAGAdroFBg51/XQzXxAAAWhozMAAAwDpNHmDmz5+vsLCwkKV///5O+6lTp5SRkaFu3bqpS5cuGjt2rCoqKkL2UV5ervT0dEVERCg6OlqzZ8/W2bNnm7qrAADAUs1yCunmm2/Whg0b/v+bdPz/bzNr1izl5eXp/fffl9fr1YwZM/Tggw/q888/lySdO3dO6enp8vl8+uKLL3To0CFNnDhRnTp10u9+97vm6C4AALBMswSYjh07yufzXbC+urpa//Ef/6EVK1bol7/8pSTp7bff1oABA7RlyxYNHz5cn3zyiXbv3q0NGzYoJiZGQ4cO1QsvvKCsrCzNnz9fLperOboMAAAs0izXwHz33XeKi4tTnz59NH78eJWXl0uSiouLdebMGaWkpDi1/fv3V69evVRUVCRJKioq0qBBgxQTE+PUpKWlKRgMateuXRd9z5qaGgWDwZAFAAC0T00eYJKTk5Wbm6t169Zp6dKlKisr01133aVjx44pEAjI5XIpMjIyZJuYmBgFAgFJUiAQCAkvde11bReTk5Mjr9frLPHx8U17YAAAoM1o8lNIY8aMcX4ePHiwkpOT1bt3b61atUqdO3du6rdzZGdnKzMz03kdDAYJMQAAtFPNfht1ZGSkbrrpJu3du1c+n0+nT59WVVVVSE1FRYVzzYzP57vgrqS61/VdV1PH7XbL4/GELAAAoH1q9gBz/Phx7du3T7GxsUpKSlKnTp1UUFDgtJeWlqq8vFx+v1+S5Pf7tWPHDlVWVjo1+fn58ng8SkxMbO7uAgAACzT5KaSnn35a9913n3r37q2DBw/queeeU3h4uB555BF5vV5NnjxZmZmZioqKksfj0ZNPPim/36/hw4dLklJTU5WYmKgJEyZo0aJFCgQCmjt3rjIyMuR2u5u6uwAAwEJNHmC+//57PfLIIzpy5Ih69OihO++8U1u2bFGPHj0kSS+//LI6dOigsWPHqqamRmlpaXrjjTec7cPDw7VmzRpNnz5dfr9f1157rSZNmqQFCxY0dVcBAIClwowxprU70RyCwaC8Xq+qq6tb5HqY878f6GrCdyEBAJrK5f795ruQAACAdfg2alyx+mafmJUBADQnZmAAAIB1mIFBszh/VoYZGQBAU2IGBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdbgLCS2CZ8UAAJoSMzAAAMA6BBgAAGAdTiGh1fCwOwBAYzEDAwAArMMMDNoMLvQFAFwuZmAAAIB1CDAAAMA6BBgAAGAdAgwAALAOF/GiTeNWawBAfZiBAQAA1mEGBlbhVmsAgMQMDAAAsBABBgAAWIdTSGh3OM0EAO0fAQbWqy+wAADaN04hAQAA6zADg6sCz5MBgPaFGRgAAGAdZmBwVeJCXwCwGwEG+H84zQQA9iDAABfBLA0AtF0EmEbgtt2rF7M0ANA2EGCAK3A5szTM5ABA0yPAAE3scmbo2tpMTlvrDwBcCgEGaANacpaGU6AA2oM2HWBef/11LV68WIFAQEOGDNGrr76q22+/vbW7BbSIy5kVaUwNALQHbTbAvPfee8rMzNSyZcuUnJysV155RWlpaSotLVV0dHRrdw9ocY05NQUA7VWbfRLvSy+9pClTpuixxx5TYmKili1bpoiICL311lut3TUAANDK2uQMzOnTp1VcXKzs7GxnXYcOHZSSkqKioqJ6t6mpqVFNTY3zurq6WpIUDAabvH+1NT82+T6BtqTXrPcvWLfz+bRW6AmAq03d321jzM/WtckA89e//lXnzp1TTExMyPqYmBjt2bOn3m1ycnL0/PPPX7A+Pj6+WfoIXG28r7R2DwBcTY4dOyav13vR9jYZYBojOztbmZmZzuva2lodPXpU3bp1U1hY2BXvPxgMKj4+XgcOHJDH47ni/eHiGOuWw1i3HMa65TDWLaO5xtkYo2PHjikuLu5n69pkgOnevbvCw8NVUVERsr6iokI+n6/ebdxut9xud8i6yMjIJu+bx+Phf4gWwli3HMa65TDWLYexbhnNMc4/N/NSp01exOtyuZSUlKSCggJnXW1trQoKCuT3+1uxZwAAoC1okzMwkpSZmalJkybp1ltv1e23365XXnlFJ06c0GOPPdbaXQMAAK2szQaYhx9+WIcPH9a8efMUCAQ0dOhQrVu37oILe1uK2+3Wc889d8FpKjQ9xrrlMNYth7FuOYx1y2jtcQ4zl7pPCQAAoI1pk9fAAAAA/BwCDAAAsA4BBgAAWIcAAwAArEOAuUyvv/66rr/+el1zzTVKTk7Wl19+2dpdssr8+fMVFhYWsvTv399pP3XqlDIyMtStWzd16dJFY8eOveBBhuXl5UpPT1dERISio6M1e/ZsnT17tqUPpc3ZvHmz7rvvPsXFxSksLEwffvhhSLsxRvPmzVNsbKw6d+6slJQUfffddyE1R48e1fjx4+XxeBQZGanJkyfr+PHjITXbt2/XXXfdpWuuuUbx8fFatGhRcx9am3Opsf7tb397we/56NGjQ2oY60vLycnRbbfdpq5duyo6OloPPPCASktLQ2qa6jOjsLBQw4YNk9vtVt++fZWbm9vch9emXM5Yjxw58oLf62nTpoXUtMpYG1zSypUrjcvlMm+99ZbZtWuXmTJliomMjDQVFRWt3TVrPPfcc+bmm282hw4dcpbDhw877dOmTTPx8fGmoKDAbNu2zQwfPtz83d/9ndN+9uxZM3DgQJOSkmK+/vpr86c//cl0797dZGdnt8bhtCl/+tOfzLPPPms++OADI8msXr06pH3hwoXG6/WaDz/80HzzzTfm17/+tUlISDAnT550akaPHm2GDBlitmzZYv785z+bvn37mkceecRpr66uNjExMWb8+PFm586d5t133zWdO3c2//7v/95Sh9kmXGqsJ02aZEaPHh3ye3706NGQGsb60tLS0szbb79tdu7caUpKSsw999xjevXqZY4fP+7UNMVnxl/+8hcTERFhMjMzze7du82rr75qwsPDzbp161r0eFvT5Yz13//935spU6aE/F5XV1c77a011gSYy3D77bebjIwM5/W5c+dMXFycycnJacVe2eW5554zQ4YMqbetqqrKdOrUybz//vvOum+//dZIMkVFRcaYv/3h6NChgwkEAk7N0qVLjcfjMTU1Nc3ad5uc/0e1trbW+Hw+s3jxYmddVVWVcbvd5t133zXGGLN7924jyXz11VdOzdq1a01YWJj53//9X2OMMW+88Ya57rrrQsY6KyvL9OvXr5mPqO26WIC5//77L7oNY904lZWVRpLZtGmTMabpPjPmzJljbr755pD3evjhh01aWlpzH1Kbdf5YG/O3APNP//RPF92mtcaaU0iXcPr0aRUXFyslJcVZ16FDB6WkpKioqKgVe2af7777TnFxcerTp4/Gjx+v8vJySVJxcbHOnDkTMsb9+/dXr169nDEuKirSoEGDQh5kmJaWpmAwqF27drXsgVikrKxMgUAgZGy9Xq+Sk5NDxjYyMlK33nqrU5OSkqIOHTpo69atTs2IESPkcrmcmrS0NJWWluqHH35ooaOxQ2FhoaKjo9WvXz9Nnz5dR44ccdoY68aprq6WJEVFRUlqus+MoqKikH3U1VzNn+3nj3Wd5cuXq3v37ho4cKCys7P1448/Om2tNdZt9km8bcVf//pXnTt37oInAMfExGjPnj2t1Cv7JCcnKzc3V/369dOhQ4f0/PPP66677tLOnTsVCATkcrku+PLNmJgYBQIBSVIgEKj3v0FdG+pXNzb1jd1PxzY6OjqkvWPHjoqKigqpSUhIuGAfdW3XXXdds/TfNqNHj9aDDz6ohIQE7du3T//8z/+sMWPGqKioSOHh4Yx1I9TW1mrmzJm64447NHDgQElqss+Mi9UEg0GdPHlSnTt3bo5DarPqG2tJevTRR9W7d2/FxcVp+/btysrKUmlpqT744ANJrTfWBBi0iDFjxjg/Dx48WMnJyerdu7dWrVp11X1IoP0aN26c8/OgQYM0ePBg3XDDDSosLNSoUaNasWf2ysjI0M6dO/XZZ5+1dlfavYuN9dSpU52fBw0apNjYWI0aNUr79u3TDTfc0NLddHAK6RK6d++u8PDwC65ur6iokM/na6Ve2S8yMlI33XST9u7dK5/Pp9OnT6uqqiqk5qdj7PP56v1vUNeG+tWNzc/9/vp8PlVWVoa0nz17VkePHmX8r1CfPn3UvXt37d27VxJj3VAzZszQmjVr9Omnn6pnz57O+qb6zLhYjcfjuer+YXWxsa5PcnKyJIX8XrfGWBNgLsHlcikpKUkFBQXOutraWhUUFMjv97diz+x2/Phx7du3T7GxsUpKSlKnTp1Cxri0tFTl5eXOGPv9fu3YsSPkwz8/P18ej0eJiYkt3n9bJCQkyOfzhYxtMBjU1q1bQ8a2qqpKxcXFTs3GjRtVW1vrfFD5/X5t3rxZZ86ccWry8/PVr1+/q+6URkN8//33OnLkiGJjYyUx1pfLGKMZM2Zo9erV2rhx4wWn1JrqM8Pv94fso67mavpsv9RY16ekpESSQn6vW2WsG33571Vk5cqVxu12m9zcXLN7924zdepUExkZGXLFNX7eU089ZQoLC01ZWZn5/PPPTUpKiunevbuprKw0xvztlshevXqZjRs3mm3bthm/32/8fr+zfd1teqmpqaakpMSsW7fO9OjRg9uojTHHjh0zX3/9tfn666+NJPPSSy+Zr7/+2vzP//yPMeZvt1FHRkaajz76yGzfvt3cf//99d5Gfcstt5itW7eazz77zNx4440ht/ZWVVWZmJgYM2HCBLNz506zcuVKExERcVXd2mvMz4/1sWPHzNNPP22KiopMWVmZ2bBhgxk2bJi58cYbzalTp5x9MNaXNn36dOP1ek1hYWHIrbs//vijU9MUnxl1t/bOnj3bfPvtt+b111+/6m6jvtRY79271yxYsMBs27bNlJWVmY8++sj06dPHjBgxwtlHa401AeYyvfrqq6ZXr17G5XKZ22+/3WzZsqW1u2SVhx9+2MTGxhqXy2V+8YtfmIcfftjs3bvXaT958qT5x3/8R3PdddeZiIgI8w//8A/m0KFDIfvYv3+/GTNmjOncubPp3r27eeqpp8yZM2da+lDanE8//dRIumCZNGmSMeZvt1L/n//zf0xMTIxxu91m1KhRprS0NGQfR44cMY888ojp0qWL8Xg85rHHHjPHjh0Lqfnmm2/MnXfeadxut/nFL35hFi5c2FKH2Gb83Fj/+OOPJjU11fTo0cN06tTJ9O7d20yZMuWCf+gw1pdW3xhLMm+//bZT01SfGZ9++qkZOnSocblcpk+fPiHvcTW41FiXl5ebESNGmKioKON2u03fvn3N7NmzQ54DY0zrjHXY/zsAAAAAa3ANDAAAsA4BBgAAWIcAAwAArEOAAQAA1iHAAAAA6xBgAACAdQgwAADAOgQYAABgHQIMAACwDgEGAABYhwADAACsQ4ABAADW+b/bCw0/jhQ8HwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"type(X_train):\", type(X_train))\n",
        "print(\"number of training sequences: X_train.shape:\", X_train.shape)\n",
        "print(\"type(X_train[0]):\", type(X_train[0]))\n",
        "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
        "print(\"length of the second training sequence: len(X_train[1]):\",len(X_train[1]))\n",
        "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
        "len_list = [len(train) for train in X_train]\n",
        "print(\"maximum length of a training sequence:\", max(len_list))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(len_list, 100);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I-cEKUh_HM4"
      },
      "source": [
        "## Details of how the reviews are encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcOwiMUT_HM5",
        "outputId": "1d2a6c99-b4ba-4134-881d-4c8407aa3dda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
          ]
        }
      ],
      "source": [
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {key:(value+param.index_word_from) for key,value in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfl42LGCugWB",
        "outputId": "cdd2e102-bdbc-493c-f41b-442ef1becf1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(y_train): <class 'numpy.ndarray'>\n",
            "y_train.shape: (25000,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "print(\"type(y_train):\", type(y_train))\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "y_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVw65PNNuobX",
        "outputId": "6ac338ef-c089-4b88-c1f5-7d35a9827df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test.shape: (25000,)\n",
            "y_test.shape: (25000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V18OA7oQNH3c"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Sequences (represented as a list of values) in `X_train` represent the reviews.\n",
        "They can have different length $T_x$.\n",
        "To train the network we should modify them so that they all have the same length `param.T_x`.\n",
        "\n",
        "We do this by:\n",
        "- **truncating** the ones that are too long,\n",
        "- **padding-with-zeros** the ones that are too short.\n",
        "\n",
        "This can be done at the start of the sequence (`pre`) or at the end (`post`).\n",
        "\n",
        "In our use-case (rating of reviews), the decision ($\\hat{y}$) is taken after reading the whole sentence/review ${x^{<t>}}$. Therefore we will truncate and pad-with-zeroes in `pre` mode (truncate the beginning of the sequence if too long, or add zeroes add the beggining of the sequence if too short)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MniSJGDF3uQT",
        "outputId": "04c67d8b-9e63-41d7-9632-41ff824282bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8UIF_RBUkfQI"
      },
      "outputs": [],
      "source": [
        "def do_pad_sequences(sequences, required_len, truncating='pre', padding='pre'):\n",
        "    \"\"\"\n",
        "    Parameters\n",
        "    ----------\n",
        "        sequences:  numpy arrays of lists, shape=(25000,)\n",
        "        required_len:     required length of each sequence after truncating and padding\n",
        "        padding     'pre' or 'post' mode\n",
        "        truncating  'pre' or 'post' mode\n",
        "    Returns\n",
        "    -------\n",
        "        padded_sequences    numpy arrays of lists (each list has now length maxlen)\n",
        "    \"\"\"\n",
        "    if student:\n",
        "        # --- START CODE HERE (01)\n",
        "        padded_sequences = []\n",
        "        for seq in sequences:\n",
        "            # Truncate\n",
        "            if len(seq) > required_len:\n",
        "                if truncating == 'pre':\n",
        "                    seq = seq[-required_len:]\n",
        "                else:  # 'post'\n",
        "                    seq = seq[:required_len]\n",
        "            # Pad\n",
        "            if len(seq) < required_len:\n",
        "                pad_len = required_len - len(seq)\n",
        "                if padding == 'pre':\n",
        "                    seq = [0] * pad_len + seq\n",
        "                else:  # 'post'\n",
        "                    seq = seq + [0] * pad_len\n",
        "            padded_sequences.append(seq)\n",
        "        padded_sequences = np.array(padded_sequences)\n",
        "        # --- END CODE HERE\n",
        "    return padded_sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhmiHsOGoRwT",
        "outputId": "5cb02566-63c2-4a5a-c72e-96142bca32d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(X_train[0]): 100\n",
            "len(X_train[1]): 100\n",
            "X_train[0]: [1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
            "    2    8    4  107  117    2   15  256    4    2    7 3766    5  723\n",
            "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
            "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
            "    2   18    4  226   22   21  134  476   26  480    5  144   30    2\n",
            "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
            "   88   12   16  283    5   16 4472  113  103   32   15   16    2   19\n",
            "  178   32]\n"
          ]
        }
      ],
      "source": [
        "# --- truncate and pad input sequences\n",
        "X_train = do_pad_sequences(X_train, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "X_test = do_pad_sequences(X_test, required_len=param.T_x, padding='pre', truncating='pre')\n",
        "\n",
        "print(\"len(X_train[0]):\", len(X_train[0]))\n",
        "print(\"len(X_train[1]):\", len(X_train[1]))\n",
        "print(\"X_train[0]:\", X_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8IktAODF8g9"
      },
      "source": [
        "# Define training and testing functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "g45DC3JEF72A"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, data_loader, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0, 0\n",
        "    for X, y in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        hat_y = model(X)\n",
        "        loss = criterion(hat_y.squeeze(), y)\n",
        "        loss.backward() # --- SPECIFIC TO TRAINING\n",
        "        optimizer.step() # --- SPECIFIC TO TRAINING\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        predicted = (hat_y.squeeze() > 0.5).float()\n",
        "        total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "def test_one_epoch(model, data_loader, criterion):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "    total_loss, total_acc =  0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in data_loader:\n",
        "            hat_y = model(X)\n",
        "            loss = criterion(hat_y.squeeze(), y)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predicted = (hat_y.squeeze() > 0.5).float()\n",
        "            total_acc += (predicted == y).sum().item()/len(y)\n",
        "\n",
        "    return total_loss/len(data_loader), total_acc/len(data_loader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_loader, test_loader, criterion, optimizer, n_epoch):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(param.n_epoch):\n",
        "\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer)\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {train_loss }, Acc: {train_acc} \")\n",
        "\n",
        "        test_loss, test_acc = test_one_epoch(model, test_loader, criterion)\n",
        "        print(f\"\\tValidation Loss: {test_loss }, Acc: {test_acc} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j9k06bjfcypD"
      },
      "outputs": [],
      "source": [
        "# --- Convert numpy.array to torch.tensor\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "# Create TensorDatasets for train and test data\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# Create DataLoaders for train and test data\n",
        "train_loader = DataLoader(train_dataset, batch_size=param.batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=param.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlrDTuk5K65Q"
      },
      "source": [
        "# First model\n",
        "\n",
        "\n",
        "\n",
        "In the first model, you will successively\n",
        "- step-1) learn word embeddings $e^{<t>}$ of each item of the inut sequence $x^{(t)}$.\n",
        "    - This is done by learning an embedding matrix $E$. You will use the `nn.Embedding` layer in pytorch.\n",
        "    - In pytorch, the `nn.Embedding` layer does not really perform a matrix multiplication going from one-hot-encoding to embedding (it would be very costly to do that).\n",
        "    - In pytorch `nn.Embedding` is a special layer that goes directly from index-of-the-word-in-the-dictionary to the embedding $e^{(t)}$\n",
        "    - The embedding goes from `param.n_word` dimensions to  `param.n_embedding` dimensions\n",
        "- step-2) compute the average over time $t$ of the embedding $e^{(t)}$ obtained for each word $x^{(t)}$ of a sequence (you should use `torch.mean`)\n",
        "- step-3) apply a fully connected (`nn.linear` layer in pytorch) which output activation is a sigmoid (predicting the 0 or 1 rating)\n",
        "\n",
        "<img src=\"https://perso.telecom-paristech.fr/gpeeters/doc/Lab_DL_RNN_01.png\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zspaUptgtW9l",
        "outputId": "905e2b3f-4a1d-4604-8b46-494b555adc1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (02)\n",
        "    class SimpleModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super(SimpleModel, self).__init__()\n",
        "            self.embedding = nn.Embedding(param.n_word, param.n_embedding)\n",
        "            self.fc = nn.Linear(param.n_embedding, param.n_out)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)                 # x shape: (batch_size, T_x, n_embedding)\n",
        "            x = torch.mean(x, dim=1)              # mean over time axis\n",
        "            x = self.fc(x)                        # fully connected layer\n",
        "            x = self.sigmoid(x)                   # sigmoid output\n",
        "            return x\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = SimpleModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-RsBZDShCyag"
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(),param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqQJLiu2c8_M",
        "outputId": "86a10c8e-86c6-40c1-db09-530c1dd3d2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6747531950321344, Acc: 0.618486253196931 \n",
            "\tValidation Loss: 0.647277323181367, Acc: 0.6908967391304348 \n",
            "Epoch 2, Loss: 0.5944449581453563, Acc: 0.7352062020460358 \n",
            "\tValidation Loss: 0.5432630079176725, Acc: 0.7694213554987213 \n",
            "Epoch 3, Loss: 0.4897417944410573, Acc: 0.7975863171355498 \n",
            "\tValidation Loss: 0.45840003049891925, Acc: 0.8072250639386189 \n",
            "Epoch 4, Loss: 0.41904126843223183, Acc: 0.8285805626598466 \n",
            "\tValidation Loss: 0.410255674358524, Acc: 0.8243286445012787 \n",
            "Epoch 5, Loss: 0.37674824554292136, Acc: 0.8445812020460358 \n",
            "\tValidation Loss: 0.3832709690189118, Acc: 0.8325207800511509 \n",
            "Epoch 6, Loss: 0.34965070095056155, Acc: 0.8558583759590793 \n",
            "\tValidation Loss: 0.36714875137866915, Acc: 0.8377557544757033 \n",
            "Epoch 7, Loss: 0.33033660705894463, Acc: 0.8615968670076726 \n",
            "\tValidation Loss: 0.35657155182203065, Acc: 0.842391304347826 \n",
            "Epoch 8, Loss: 0.31547949533633257, Acc: 0.8678628516624042 \n",
            "\tValidation Loss: 0.34926891365014684, Acc: 0.8453884271099744 \n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBqyzLJRUIsC"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy \"around\" 86.7%/ 84.5% for the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRP-h4Xr_HNJ"
      },
      "source": [
        "## Using the trained embedding to find equivalence between words\n",
        "\n",
        "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
        "You will use the weights of the `nn.Embedding` layer to find the most similar words to `great`. We will use an Euclidean distance for that.\n",
        "\n",
        "- 1) Retrieve the weights of the `nn.Embedding` layer\n",
        "- 2) Get the position of `great` in the dictionary\n",
        "- 3) Knowing this position, get the word-embedding of `great`\n",
        "- 4) Find (using Euclidean distance), the closest embedded-words to `great`\n",
        "\n",
        "Remarks:\n",
        "- you can access a specific layer of the model by using the name you used to define `self.??? = nn.Embedding` in the `__init__`method of `SimpleModel`.\n",
        "- be careful about the order of the dimensions of the embedding matrix `E`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xMubRqJ_HNJ",
        "outputId": "19ec1bb9-9019-4f61-c960-304f8b61f8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great\n",
            "fun\n",
            "masterpiece\n",
            "excellent\n",
            "friendship\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (03)\n",
        "    E = model.embedding.weight.data\n",
        "    word = 'great'\n",
        "    idx_word = word_to_id[word]\n",
        "    emb_word = E[idx_word]\n",
        "    distances = torch.norm(E - emb_word, dim=1)\n",
        "    closest_indices = torch.topk(-distances, 5).indices\n",
        "    for idx in closest_indices:\n",
        "        print(id_to_word[idx.item()])\n",
        "    # --- END CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zK9e5Eo1Ks2a"
      },
      "source": [
        "# Second model\n",
        "\n",
        "In the second model, you will replace step-2 (which was \"compute the average over time $t$ of the embedding $e^{(t)}$\") by a RNN layer over time.\n",
        "More precisely you will use a LSTM (`nn.LSTM` layer in pytorch) with `param.n_lstm=100` units (or dimensions) in a Many-To-One configuration\n",
        "\n",
        "Don't forget that in ou data, the first dimension of `X_train/X_test` represents the batch (`batch_first=True` in pytorch).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dl-CSMKoViX",
        "outputId": "c81ee5b2-a4d2-4cb6-88f6-4fd06a6ba68e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 100])\n",
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "if student:\n",
        "    # --- START CODE HERE (04)\n",
        "    class LstmModel(nn.Module):\n",
        "        def __init__(self, param):\n",
        "            super(LstmModel, self).__init__()\n",
        "            self.embedding = nn.Embedding(param.n_word, param.n_embedding)\n",
        "            self.lstm = nn.LSTM(input_size=param.n_embedding, hidden_size=param.n_lstm, batch_first=True)\n",
        "            self.fc = nn.Linear(param.n_lstm, param.n_out)\n",
        "            self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        def forward(self, x):\n",
        "            x = self.embedding(x)\n",
        "            _, (h_n, _) = self.lstm(x)\n",
        "            x = self.fc(h_n[-1])\n",
        "            x = self.sigmoid(x)\n",
        "            return x\n",
        "    # --- END CODE HERE\n",
        "\n",
        "\n",
        "# --- Test\n",
        "torch.manual_seed(0)\n",
        "model = LstmModel(param)\n",
        "print(X_train_tensor[:param.batch_size, :].size())\n",
        "print(model(X_train_tensor[:param.batch_size, :]).size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-bp7PzX7oXtB",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), param.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP8TvdgNiUQd",
        "outputId": "2ab989c5-41ca-4db6-8cc5-7b130c69e990"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.633679269982116, Acc: 0.6224904092071611 \n",
            "\tValidation Loss: 0.5546997210101399, Acc: 0.713618925831202 \n",
            "Epoch 2, Loss: 0.47997121874938536, Acc: 0.7688698849104859 \n",
            "\tValidation Loss: 0.4312525246759205, Acc: 0.7994645140664961 \n",
            "Epoch 3, Loss: 0.39649670420552763, Acc: 0.821491368286445 \n",
            "\tValidation Loss: 0.43187597615029805, Acc: 0.7988331202046036 \n",
            "Epoch 4, Loss: 0.3494471668663537, Acc: 0.8509830562659847 \n",
            "\tValidation Loss: 0.3876821159020714, Acc: 0.8229699488491049 \n",
            "Epoch 5, Loss: 0.3041649828938877, Acc: 0.8731377877237853 \n",
            "\tValidation Loss: 0.38354839426477244, Acc: 0.8402493606138108 \n",
            "Epoch 6, Loss: 0.2741476274893412, Acc: 0.8868366368286446 \n",
            "\tValidation Loss: 0.3616966738572816, Acc: 0.8408168158567775 \n",
            "Epoch 7, Loss: 0.24274136888249145, Acc: 0.9036125319693095 \n",
            "\tValidation Loss: 0.37307657392890864, Acc: 0.8496163682864449 \n",
            "Epoch 8, Loss: 0.21970655426116245, Acc: 0.9140425191815856 \n",
            "\tValidation Loss: 0.4083268909007692, Acc: 0.8468430306905371 \n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "train(model, train_loader, test_loader, criterion, optimizer, param.n_epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1LN_fjMWBHJ"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 8 epochs, you should obtain an accuracy around 91.1%/ 84.7% for the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qBjoRpoqv_g"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for\n",
        "- 1) Data Pre-Processing (01)\n",
        "- 2) Simple model  (02)\n",
        "- 3) Find equivalence between words (03)\n",
        "- 4) LSTM model (04)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 1) Simple but effective implementation for sequence padding and truncation with flexible pre/post options, well-suited for text.\n",
        "- 2) minimalist architecture, but well implimented , gave the correct results,\n",
        "Good starting baseline, but may underperform on complex language structures.\n",
        "- 3) correct results, Could be improved by normalizing embeddings and using cosine similarity\n",
        "- 4)Good improvement over the first model by introducing an LSTM for temporal modeling; correct usage of many-to-one configuration for classification."
      ],
      "metadata": {
        "id": "9nHIZ2gyMR8q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "UnwJcS93qv_g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}